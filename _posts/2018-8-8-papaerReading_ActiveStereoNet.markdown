---
layout: 	post
title: 		"论文阅读《ActiveStereoNet:End-to-End Self-Supervised Learning for Active Stereo Systems》"
subtitle:   ""
date:       2018-08-08 12:30:00
author:     "stonlimart"
#header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
	- StereoVision
---


> 本文出自谷歌与普林斯顿大学研究人员之手并发表于计算机视觉顶会ECCV2018。本文首次提出了应用于主动双目立体视觉的深度学习解决方案，并引入了一种新的重构误差，采用自监督的方法来解决缺少ground truth数据的问题，本文所提供的方法在许多方面表现出了最好的结果

##  Abstract

本文首次提出了第一个主动双目视觉系统的深度学习解决方案 ActiveStereoNet。由于缺乏 ground truth，本文采用了完全自监督的方法，即使如此，本方法也产生了 1/30 亚像素精度的深度数据。它克服了过度平滑的问题，保留了边缘，并且能有效处理遮挡。在本文中，引入了一种对噪声，无纹理区域和光照更加鲁棒的新的重建损失，它使用基于窗口的自适应支持权重的成本聚合来进行优化。这种成本聚合函数能够保留边缘并且使损失函数平滑，是使模型达到惊人结果的关键。最后，本文展示了如何在没有ground truth下对无效的区域，遮挡区域的预测进行end-to-end的训练。这部分对减少模糊和改善预测数据的连续性至关重要。 

## Introduction

**深度传感器（Depth sensors）**为许多难题提供了额外的3D信息，如非刚性重构（non-rigid reconstruction）、动作识别和参数跟踪，从而给计算机视觉带来了革新。虽然深度传感器技术有许多类型，但它们都有明显的局限性。例如，飞行时间系统（Time of flight systems）容易遭受运动伪影和多路径的干扰，结构光（structured light ）容易受到环境光照和多设备干扰。在没有纹理的区域，需要昂贵的全局优化技术，特别是在传统的非学习方法中，passive stereo很难实现。

**主动双目立体视觉（Active stereo）**提供了一种潜在的解决方案：使用一对红外立体相机，使用一个伪随机模式，通过图案化的红外光源对场景进行纹理化（如图1所示）。通过合理选择传感波长，相机对捕获主动照明和被动光线的组合，提高了结构光的质量，同时在室内和室外场景中提供了强大的解决方案。虽然这项技术几十年前就提出了，但直到最近才出现在商业产品中。因此，从主动双目立体图像中推断深度的先前工作相对较少，并且尚未获得大规模的ground truth训练数据。 

在主动双目立体成像系统中必须解决几个问题。有些问题是所有的双目系统问题共有的，例如，必须避免匹配被遮挡的像素，这会导致过度平滑、边缘变厚和/或轮廓边缘附近出现飞行像素。但是，其他一些问题是主动双目系统特有的，例如，它必须处理非常**高分辨率的图像**来匹配投影仪产生的高频模式；它必须避免由于这些高频模式的其他排列而产生的许多**局部最小值**；而且它还必须**补偿附近和远处表面投影图案之间的亮度差异**。此外，它不能接受ground truth深度的大型主动双目数据集的监督，因为**没有可用的数据**。

在这篇论文中，我们第一个提出了针对主动立体视觉系统的端到端深度学习方案。首先，本文提出了一种基于**局部对比度归一化（local contrast normalization）**的新的重建损耗，其从被动IR图像中去除低频分量并且局部地重新校准有源图像的强度以解决有源立体视觉系统中能量随距离的衰落的问题。 第二，我们提出了一种**基于窗口的损耗聚合**，其中每个像素具有自适应权重，以增加其可辨识性并降低立体声成本函数中局部最小值的影响。 最后，我们检测图像中的遮挡像素，并从损失计算中忽略它们。 这些新特性为训练期间的提高了收敛速度，并在测试时提高了预测的精度。大量实验表明，我们使用这些想法进行训练的网络优于以往有关主动立体匹配的研究。

## Method

现在来介绍ActiveStereoNet的网络结构和主要训练过程。

算法的输入是一对**矫正过的同步采集的**IR图像, 输出是一对同原始分辨率的视察图像。在本实验中采用1280*720的图像。相机的焦距$$f$$和两个相机间的基线$$b$$假设已知。因此，深度的预测问题转化为了同一扫描线上的视差问题。给定视差 $$d$$，则深度是 $$Z = \frac{bf}{d}$$

因为缺乏ground truth数据，本算的主要挑战是在没有直接监督的情况下，训练一个对遮挡和光照变化鲁棒的端对端网络。下面是算法细节：

### Network Architecture

现在，在大部分视觉任务中，网络结构的选择和设计是最重要的，需要花费大量的精力。但是在本文中，最重要的问题是训练一个给定的网络结构，特别是，因为是无监督的网络，设计一个好的损失函数对总体的精度影响最大。因此，在本文中采用了谷歌另一篇论文 [StereoNet](https://arxiv.org/abs/1807.08865) 的网络结构，这个结构在被动的双目视觉系统中有很好的表现。而且，该结构在高配GPU下能对全分辨图像做到60HZ, 能达到实时性的要求。

整体的流程图如图所示：

> ![](../img/in-post/papers/ActiveStereo-network.png)算法采用两阶段模型，  首先建立一个低分辨率的成本项并进行的第一次视差估计。然后进行双线性采样，其后跟一个残差网络来预测最终的误差。右下的 “有效性网络” 也是通过端到端训练来预测置信度。

本文从一个高分辨率首先使用Siamese双塔来产生1/8分辨率的特征图，而后建立一个160x90x18的低分辨率的成本项，这个成本项允许在原始分辨率下最大144的视差，大概距离镜头30厘米的距离。

该成本项通过使用**soft argmin operator**产生一个下采样的视差图，而后采用双线性插值来得到原始分辨率的视差图，后面接一个残差微调模块来检索如边缘之类的高频细节。微调模块首先分别在上采样视差图和输入图像上运行单独的卷积层，并将其与特征合并来产生残差，这在实践中更有效地去除最新的结果中的点伪影。

### Loss Function

上述网络结构是由一个低分辨率的视差网络和后续的微调模块来恢复高频细节，所以自然想到的方案是每个步骤都有一个损失函数。但是由于是无监督的，所以一个可行的方案是用左图和重构的左图之间的光度误差来作为损失函数 $$L = \sum_{ij}||I_{ij}^l - \hat{I}_{ij}^l||_1$$ ,重构的左图是通过预测视差和右图采样得到的。采样器是使用**Spatial Transformer Network（STN）** 得到的，该采样器对同行的2个像素进行双线性采样而且是完全可微的。

但是光度损失在图像重建中表现很差，尤其在主动双目系统中。首先，接受到的信号强度与距离的平方成反比，即 $I\propto\frac{1}{Z^2}$，这导致光度对距离的明显的依赖。第二，明亮的像素比暗点的像素更容易产生较大的残差，因为 $I = I^{*}+N(0,\sigma_1I^*+\sigma_2)$, 则两个匹配像素之间的残差满足分布$\epsilon  = N(0,\sqrt{(\sigma_1I^*+\sigma_2)^2+(\sigma_3\hat{I}^*+\sigma_4)^2} )$，可知，方差与亮度有关。在主动双目系统中，上述问题会使得网络偏向特殊的场景，永远会有更大的误差。网络会在更容易的区域学习，然后对其它区域进行平滑。那些较暗的像素，需要更高的精确度来确定深度，却会被忽略掉。

而且，当前景和背景差异很大时，这种损失会在被遮挡区域产生更大的误差，导致网络再无法解释的区域学习。

**加权局部对比度归一化.**  本文提出使用局部对比度归一化方法，它不仅移除了亮度和视差的相关性，而且再遮挡区域给出了更好的残差。它还对左右图的亮度变化具有不变性。 特别是，对于每个像素，计算一个9*9的小像素块中的局部均值$\mu$和标准差$\sigma$， 局部的统计量用于归一化当前像素的亮度$I_{LCN} = \frac{I-\mu}{\sigma+\eta}$ ,$\eta$是一个小常数。下图表现了LCN归一化的结果

> ![](../img/in-post/papers/ActiveStereo-LCN.png)

但是，LCN在标准差接近0的弱纹理的区域表现不好（Fig.3 LCN error）. 事实上是因为这些区域小的$\sigma$会放大任何两个匹配像素间残差和残差。为了移除这些影响，本文使用局部标准差$\sigma_{ij}$来对两个匹配像素进行加权。因此，重建误差就变为了$L = \sum_{ij}||\sigma_{ij}(I_{LCN}^l - \hat{I}_{LCN}^l)||_1 = \sum_{ij}C_{ij}$.

### Window-based Optimization

本段具体分析下损失函数在搜索空间中的表现。对于单个像素成本，该曲线是高度非凸的，这使得很难检索到真实值。事实上，该曲线有许多局部极值，导致很难达到真正的最优。在传统的立体匹配中，成本聚合使用来自相邻像素的证据来保证最优估计。如果我们考虑每个像素周围的窗口中的成本之和，则损失函数在有纹理和无纹理区域都会变得平滑，而且能够达到最优值。但是，大窗口的缺点是 小的物体和细节会被多个成本聚合而平滑掉，并且在最后的视差中无法恢复。

传统的立体匹配使用自适应的支持窗口**(ASW)**,进行成本聚合，这虽然有效但是却非常慢，在实时系统中无法使用。在这里，本文提议在训练工程中使用 ASW 方案，因此他不影响运行时的成本。考虑一个$(i,j)$处的亮度为$I_{ij}$像素，与其计算每个像素的Loss，在这里使用一个2Kx2K窗口大小的聚合成本: $\hat{C}_{ij} = \frac{\sum_{x = i-k}^{i+k-1}\sum_{y = j-k}^{j+k-1}w_{xy}C_{ij}}{\sum_{x = i-k}^{i+k-1}\sum_{y = j-k}^{j+k-1}w_{xy}}$, $W_{xy} = \exp(-\frac{|I_{ij}-I{xy}|}{\sigma_w})$, $\sigma_w = 2$

> ![](../img/in-post/papers/ActiveStereo-ASW.png)

### Invalidation Network

上述中的loss并没有处理遮挡和错误匹配。遮挡区域并不能在成本项中提供任何有效的信息。为解决遮挡问题，传统的立体匹配方法采用左右一致性校验方法即，将左右视差差值小于某个阈值 $|d_l-d_r| > \theta$剔除。

相反，本文提议定义一个对像素$(i,j)$的mask : $m_{ij} = |d_l-d_r| < \theta$ 的左侧检查作为硬性条件限制，$\theta = 1$。 $m_{ij} = 0$的像素在loss计算中会被忽略。为了避免（即所有像素都无效）的无脑情况，我们通过在每个像素位置中使用恒定标签1最小化交叉熵损失来对有效像素的数量实施正则化。我们在低分辨率差异和最终分辨率差异中使用此掩码。 

同时，本结构训练一个全卷积的无效性网络，将Siamese塔的特征图作为输入，产生一个低分辨率的无效区域mask, 然后将其上采样到并且使用微调模块进行微调。这个网络避免在运行时对左右一致性进行校验，让推断更快。

## Experiments

本文进行了一系列实验来对模型进行评估，除了分析深度预测的准确性，并将其与以前的成果相比之外，我们还提供消融研究的结果，以研究拟损失的每个组成部分会对结果造成什么影响。在补充材料中，我们还评估了我们提出的self-supervised loss 方法在passive (RGB) stereo中的适用性，该方案表现出更高的泛化能力，在许多基准测试中达到了令人印象深刻的结果。

### 立体匹配评估

在本节中，使用传统的双目立体匹配指标（如抖动和偏差），定性、定量地将我们的方法在实际数据的实验中与最先进的立体算法进行比较。

**抖动与偏差**  假设某立体匹配系统的基线标准为b，焦距为f，子像素视差精度为δ，则视差精度的深度误差e与深度Z的平方成正比。由于视差误差对深度的影响是可变的，一些简单的评估度量（如视差的平均误差）不能有效地反映估计深度的质量。而我们的方法首先标出深度估计的误差，然后计算视差中的相应误差。

为了评估ASN的子像素精度，我们记录了相机在平坦的墙壁前记录的100帧图像，相机距离墙壁的范围从500毫米到3500毫米不等，还有100帧，然后让相机成50度角朝向墙壁，再记录100帧，用来评估倾斜表面上的图像。在本例中，我们将得到的结果与高鲁棒性的平面拟合获得的“ground truth”进行对比评估 。

实验结果显示本文方法的数据偏差降低了一个数量级，子像素精度为0.03像素，而且抖动非常低。我们还展示了距离墙壁3000毫米时，多种方案下预计出现的点云。请注意，尽管距离较远（3米），但其他方法相比，我们的结果噪音更低。如图所示：

> ![](../img/in-post/papers/ActiveStereo-wall.png)

为了表示精度，我们将偏差计算为预测深度和真实值之间的平均误差l1。图中所示为关于我们所用的方法的深度偏差和传感器输出、现有最佳技术的局部立体化方法（PatchMatch，HashMatch），以及我们所使用的最先进的非监督式训练出的模型，并对点云做了表面法线着色处理的可视化操作。我们的系统在距墙壁全部距离上的性能都明显优于其他方法，并且其误差不会随着深度增加而显着增加。我们系统对应的子像素视差精度为1/30像素，这是通过使用上述方程（也在图5中给出）拟合曲线而获得的。这比其他方法的精度（不高于0.2像素）精确一个数量级。

为了表示噪声，我们将抖动（Jitter）计算为深度误差的标准偏差。如上图表明，与其他方法相比，我们的方法在几乎每个深度上都能实现最低的抖动。

**与最新技术比较** 在具有挑战性的场景中对ASN的更多定性评估如图所示。可以看出，像PatchMatch和HashMatch这样的局部方法无法处理有源光和无源光的混合照明场景，因此会产生不完整的差异图像（缺失像素显示为黑色）。使用半全局方案的传感器输出更适合此类数据，但仍然容易受到图像噪声的影响（请注意第四列中的噪声结果）。

> ![](../img/in-post/papers/ActiveStereo-soa.png)



关于真实序列的更多例子如图8（右）所示，其中我们给出了由表面法线着色的点云。我们的输出保留了所有细节，噪音很低。相比之下，我们使用自监督方法进行训练的网络产生了过度平滑的输出。

## Conclusion

我们在本文中介绍了ActiveStereoNet（ASN），这是第一种用于主动双目立体成像系统的深度学习方法。我们设计了一个新的损耗函数来处理高频模式，照明效果和像素遮挡的情况，以解决自我监督设置中的主动立体声问题。我们的方法能够进行非常精确的重建，子像素精度达到0.03像素，比其他有源立体匹配方法精确一个数量级。与其他方法相比，ASN不会产生过于平滑的细节，可以生成完整的深度图，保留有清晰的边缘，没有乱飞的像素。而失效网络作为一个副产物，能够得出可用于需要遮挡处理的高级应用的视差置信度图。大量实验显示，使用NVidia Titan X显卡和最先进的方法，用于不同具有挑战性场景的处理任务，每帧运行平均时间为15ms.

 